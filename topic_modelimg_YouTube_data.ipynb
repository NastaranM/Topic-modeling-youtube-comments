{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Libraris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "from wordcloud import WordCloud\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref:https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf\n",
    "# ref:https://github.com/wjbmattingly/topic_modeling_textbook/blob/main/03_03_lda_model_demo_bigrams_trigrams.ipynb\n",
    "# ref:http://topic-modeling.pythonhumanities.com/01_02_topics_and_clusters.html\n",
    "from gensim import models\n",
    "def make_bigrams(texts):\n",
    "    bigram = models.Phrases(texts, min_count=5, threshold=20)\n",
    "    bigram = models.phrases.Phraser(bigram)\n",
    "    return([bigram[doc] for doc in texts])\n",
    "def make_trigrams(texts):\n",
    "    bigram = models.Phrases(texts, min_count=5, threshold=20)\n",
    "    bigram = models.phrases.Phraser(bigram)\n",
    "    trigram = models.Phrases(bigram[texts], threshold=20)\n",
    "    trigram = models.phrases.Phraser(trigram)\n",
    "    return ([trigram[bigram[doc]] for doc in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://datascience.oneoffcoder.com/topic-modeling-gensim.html\n",
    "# ref: https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(doc):\n",
    "    my_token = []\n",
    "    for text in nlp.pipe(doc, disable=[\"tagger\", \"parser\", \"ner\",\"textcat\"]):\n",
    "        tokens = [token.lemma_.lower() for token in text if \n",
    "                  token.is_alpha and\n",
    "                  not token.is_stop and \n",
    "                  not token.is_punct and\n",
    "                  len(token) >= 3]\n",
    "        my_token.append(tokens)\n",
    "    res = [elements for elements in my_token if elements != []]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "def dictionary(doc):\n",
    "    my_dict = corpora.Dictionary(doc)\n",
    "    # removed the stop words in lemmatization\n",
    "    bad_ids = [i for t, i in my_dict.token2id.items() if nlp.vocab[t].is_stop]\n",
    "    my_dict.filter_tokens(bad_ids=bad_ids)\n",
    "    my_dict.filter_extremes(no_below=20, no_above=0.5)\n",
    "    return my_dict\n",
    "\n",
    "\n",
    "# convert list of tokens to bag of word representation\n",
    "def corpus(id2word,lemmatized_doc):\n",
    "    my_corpus = [id2word.doc2bow(doc) for doc in lemmatized_doc]\n",
    "    my_corpus = [elements for elements in my_corpus if elements != []]\n",
    "    return my_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preparing comments data\n",
    "GB_videos = pd.read_csv(\"GBvideos.csv\",on_bad_lines='skip')\n",
    "GB_comments = pd.read_csv(\"GBcomments.csv\",on_bad_lines='skip')\n",
    "\n",
    "GB_videos.info()\n",
    "GB_videos = GB_videos.dropna()\n",
    "med_GB_videos=GB_videos.sample(50,random_state=10,)\n",
    "med_GB_comments = GB_comments[GB_comments['video_id'].isin(med_GB_videos['video_id'])]\n",
    "med_GB_comments.info()\n",
    "set(med_GB_comments['video_id'].unique()).issubset(set(med_GB_videos['video_id'].unique()))\n",
    "print(med_GB_videos['category_id'].unique())\n",
    "len(med_GB_videos['category_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_GB_comments = med_GB_comments.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_GB_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_string = ','.join(list(med_GB_comments['comment_text'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(med_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocssing comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "dataset = preprocess(med_GB_comments['comment_text'].astype(str))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_trigrams(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_dict = dictionary(dataset)\n",
    "med_corpus = corpus(med_dict,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(med_dict))\n",
    "print('Number of documents: %d' % len(med_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "med_word_list = ','.join(str(elem) for elem in dataset)\n",
    "# Join the different processed titles together.\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(med_word_list)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec-LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "start_time = time.time()\n",
    "# Train the Word2Vec model\n",
    "w2v_model = models.word2vec.Word2Vec(dataset, window=5, min_count=5,seed=10)\n",
    "\n",
    "# Get the word vectors from the Word2Vec model\n",
    "word_vectors = w2v_model.wv.vectors\n",
    "\n",
    "normalized_word_vectors = normalize(word_vectors)\n",
    "coherence_scores = []\n",
    "\n",
    "num_clusters = range(5,26,5)\n",
    "for cluster in num_clusters:\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster, \n",
    "                             init='k-means++',\n",
    "                             n_init=10, \n",
    "                             batch_size=100,\n",
    "                             random_state=10)\n",
    "\n",
    "    cluster_labels = kmeans.fit(normalized_word_vectors)\n",
    "    cluster_labels = kmeans.predict(normalized_word_vectors)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5 #change the clusters to 10,15,20 and get the topics\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_clusters, \n",
    "                             init='k-means++',\n",
    "                             n_init=10, \n",
    "                             batch_size=100,\n",
    "                             random_state=10)\n",
    "\n",
    "cluster_labels = kmeans.fit(normalized_word_vectors)\n",
    "cluster_labels = kmeans.predict(normalized_word_vectors)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "transformed_vectors = kmeans.transform(normalized_word_vectors)\n",
    "\n",
    "# Find the indices of the words closest to each cluster center and print them\n",
    "for i in range(num_clusters):\n",
    "    closest_indices = np.argsort(transformed_vectors[:, i])[:10]\n",
    "    closest_words = [w2v_model.wv.index_to_key[idx] for idx in closest_indices]\n",
    "    print(f\"Retrieved top words for cluster {i}: {closest_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-cut-weighted NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "start_time = time.time()\n",
    "#ref:https://stackoverflow.com/questions/45547568/how-can-i-prevent-tfidfvectorizer-to-get-numbers-as-vocabulary\n",
    "vectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1, 3),token_pattern =r'\\b[A-Za-z]{3,}\\b')\n",
    "tfidf = vectorizer.fit_transform(med_GB_comments['comment_text'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# calculate cosine similarity matrix\n",
    "similarity = cosine_similarity(tfidf)\n",
    "# compute N-cut weights using the cosine similarity matrix\n",
    "ncut_weights = np.exp(-(1 - similarity))\n",
    "\n",
    "topics = range(5,21,5)\n",
    "\n",
    "uci_coherence_scores = []\n",
    "umass_coherence_scores = []\n",
    "for num_topics in topics:\n",
    "    nmf_model = NMF(n_components=num_topics,\n",
    "                    init='nndsvd',\n",
    "                    random_state=10)\n",
    "\n",
    "    weights = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "    # normalize the topic weights\n",
    "    normalized_weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # multiply the topic weights with the N-cut weights to get the final topic weights\n",
    "    ncut_topic_weights = np.dot(normalized_weights.T, ncut_weights)\n",
    "\n",
    "    terms = np.argsort(nmf_model.components_)[:, ::-1][:, :10]\n",
    "    terms_words = [[vectorizer.get_feature_names_out()[i] for i in term] for term in terms]\n",
    "    coherence_model = CoherenceModel(topics=topic_terms_words,\n",
    "                                      corpus=med_corpus,\n",
    "                                      dictionary=med_dict,\n",
    "                                      coherence='c_uci',\n",
    "                                      texts=dataset)\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    uci_coherence_scores.append(coherence)\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topic_terms_words,\n",
    "                                 corpus=med_corpus,\n",
    "                                 dictionary=med_dict,\n",
    "                                 coherence='u_mass')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    umass_coherence_scores.append(coherence)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5 # change to 10,15,20 to get the topics\n",
    "uci_coherence_scores = []\n",
    "umass_coherence_scores = []\n",
    "nmf_model = NMF(n_components=num_topics,\n",
    "                init='nndsvd',\n",
    "                random_state=10)\n",
    "\n",
    "weights = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "# normalize the topic weights\n",
    "normalized_weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "# multiply the topic weights with the N-cut weights to get the final topic weights\n",
    "ncut_topic_weights = np.dot(normalized_weights.T, ncut_weights)\n",
    "\n",
    "terms = np.argsort(nmf_model.components_)[:, ::-1][:, :10]\n",
    "terms_words = [[vectorizer.get_feature_names_out()[i] for i in term] for term in terms]\n",
    "coherence_model = CoherenceModel(topics=topic_terms_words,\n",
    "                                  corpus=med_corpus,\n",
    "                                  dictionary=med_dict,\n",
    "                                  coherence='c_uci',\n",
    "                                  texts=dataset)\n",
    "coherence = coherence_model.get_coherence()\n",
    "uci_coherence_scores.append(coherence)\n",
    "\n",
    "coherence_model = CoherenceModel(topics=topic_terms_words,\n",
    "                             corpus=med_corpus,\n",
    "                             dictionary=med_dict,\n",
    "                             coherence='u_mass')\n",
    "coherence = coherence_model.get_coherence()\n",
    "umass_coherence_scores.append(coherence)\n",
    "print(f\"Retrieved top words for cluster {num_topics}: {terms_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
